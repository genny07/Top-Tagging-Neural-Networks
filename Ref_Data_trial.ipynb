{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with dataset, no useful DNN in this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import seaborn as sb\n",
    "import matplotlib as mpl\n",
    "import h5py\n",
    "import pandas as pd \n",
    "import tables\n",
    "from pandas.compat import StringIO, BytesIO \n",
    "\n",
    "mpl.rcParams['figure.dpi']= 250\n",
    "pd.__file__\n",
    "\n",
    "from torch.utils import data\n",
    "#from hdf5dataset import HDF5Dataset\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/', '/table']\n"
     ]
    }
   ],
   "source": [
    "#open hdf5 file for reading\n",
    "hdf_train = pd.HDFStore('train.h5',mode='r')\n",
    "print(hdf_train.keys())\n",
    "table_train = hdf_train.get('/table')\n",
    "\n",
    "#hdf_val = pd.HDFStore('val.h5',mode='r')\n",
    "#hdf_val.keys()\n",
    "#table_val = hdf.get('/table')\n",
    "\n",
    "#hdf_test = pd.HDFStore('test.h5',mode='r')\n",
    "#hdf_test.keys()\n",
    "#table_test = hdf.get('/table')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806\n",
      "E_0\n",
      "is_signal_new\n",
      "PZ_3\n",
      "1211000\n"
     ]
    }
   ],
   "source": [
    "#type(table) DataFrame type\n",
    "print(len(table_train.columns))\n",
    "print(table_train.columns[0]) #E_0\n",
    "print(table_train.columns[805]) #last column \"is signal new\" 1 = top, 0 = qcd\n",
    "print(table_train.columns[15])\n",
    "\n",
    "print(len(table_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_signal_new\n"
     ]
    }
   ],
   "source": [
    "print(table_train.columns[-1])\n",
    "X = table_train[table_train.columns[0:800]]\n",
    "Y = table_train[table_train.columns[805]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375    0\n",
       "377    0\n",
       "378    0\n",
       "379    0\n",
       "380    0\n",
       "383    0\n",
       "384    0\n",
       "386    0\n",
       "389    0\n",
       "391    0\n",
       "351    0\n",
       "352    0\n",
       "353    0\n",
       "355    0\n",
       "356    0\n",
       "358    0\n",
       "359    0\n",
       "360    0\n",
       "361    0\n",
       "363    0\n",
       "193    0\n",
       "195    0\n",
       "196    0\n",
       "198    0\n",
       "199    0\n",
       "200    0\n",
       "202    0\n",
       "204    0\n",
       "205    0\n",
       "206    0\n",
       "      ..\n",
       "162    1\n",
       "163    1\n",
       "164    1\n",
       "165    1\n",
       "168    1\n",
       "170    1\n",
       "171    1\n",
       "172    1\n",
       "174    1\n",
       "175    1\n",
       "790    1\n",
       "791    1\n",
       "792    1\n",
       "793    1\n",
       "794    1\n",
       "795    1\n",
       "797    1\n",
       "798    1\n",
       "799    1\n",
       "801    1\n",
       "576    1\n",
       "579    1\n",
       "582    1\n",
       "585    1\n",
       "586    1\n",
       "587    1\n",
       "591    1\n",
       "592    1\n",
       "593    1\n",
       "596    1\n",
       "Name: is_signal_new, Length: 1211000, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211000, 806)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create t\n",
    "train_tensor = torch.tensor(table_train.values)\n",
    "#val_tensor = torch.tensor(table_val.values)\n",
    "#test_tensor = torch.tensor(table_test.values)\n",
    "\n",
    "sub_train_tensor = train_tensor[0:15]\n",
    "\n",
    "#len(torch_tensor) 1211000\n",
    "\n",
    "# another option, but this appears to be the same as using table...\n",
    "# d = pd.read_hdf('train.h5','table')\n",
    "# torch_tensor = torch.tensor(d.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_tensor,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "sub_train_loader = torch.utils.data.DataLoader(dataset=sub_train_tensor,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):\n",
    "        super(NetBatchNorm, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_size, n_hidden1)\n",
    "        self.linear2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.linear3 = nn.Linear(n_hidden2, out_size)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden1)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden2)\n",
    "        \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(torch.sigmoid(self.linear1(x)))\n",
    "        x = self.bn2(torch.sigmoid(self.linear2(x)))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "    # Activations, to analyze results \n",
    "    def activation(self, x):\n",
    "        out = []\n",
    "        z1 = self.bn1(self.linear1(x))\n",
    "        out.append(z1.detach().numpy().reshape(-1))\n",
    "        a1 = torch.sigmoid(z1)\n",
    "        out.append(a1.detach().numpy().reshape(-1).reshape(-1))\n",
    "        z2 = self.bn2(self.linear2(a1))\n",
    "        out.append(z2.detach().numpy().reshape(-1))\n",
    "        a2 = torch.sigmoid(z2)\n",
    "        out.append(a2.detach().numpy().reshape(-1))\n",
    "        return out\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_size, n_hidden1)\n",
    "        self.linear2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.linear3 = nn.Linear(n_hidden2, out_size)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "    # Activations, to analyze results \n",
    "    def activation(self, x):\n",
    "        out = []\n",
    "        z1 = self.linear1(x)\n",
    "        out.append(z1.detach().numpy().reshape(-1))\n",
    "        a1 = torch.sigmoid(z1)\n",
    "        out.append(a1.detach().numpy().reshape(-1).reshape(-1))\n",
    "        z2 = self.linear2(a1)\n",
    "        out.append(z2.detach().numpy().reshape(-1))\n",
    "        a2 = torch.sigmoid(z2)\n",
    "        out.append(a2.detach().numpy().reshape(-1))\n",
    "        return out \n",
    "    \n",
    "    \n",
    "def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n",
    "    i = 0\n",
    "    useful_stuff = {'training_loss':[], 'validation_accuracy':[]}  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x.view(-1, 28 * 28))\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            useful_stuff['training_loss'].append(loss.data.item())\n",
    "            \n",
    "        correct = 0\n",
    "        for x, y in validation_loader:\n",
    "            model.eval()\n",
    "            yhat = model(x.view(-1, 28 * 28))\n",
    "            _, label = torch.max(yhat, 1)\n",
    "            correct += (label == y).sum().item()\n",
    "            \n",
    "        accuracy = 100 * (correct / len(validation_dataset))\n",
    "        useful_stuff['validation_accuracy'].append(accuracy)\n",
    "    \n",
    "    return useful_stuff\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Data Loader for both train and validating\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_tensor, batch_size=2000, shuffle=True)\n",
    "#validation_loader = torch.utils.data.DataLoader(dataset=val_tensor, batch_size=5000, shuffle=False)\n",
    "\n",
    "\n",
    "sub_train_loader = torch.utils.data.DataLoader(dataset=sub_train_tensor,\n",
    "                                           batch_size=2000, \n",
    "                                           shuffle=True)\n",
    "\n",
    "type(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the criterion function\n",
    "criterion = nn.CrossEntropyLoss\n",
    "\n",
    "# Set the parameters\n",
    "input_dim = 12\n",
    "hidden_dim = 100\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, optimizer and train the model\n",
    "\n",
    "model_norm  = NetBatchNorm(input_dim, hidden_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(model_norm.parameters(), lr = 0.1)\n",
    "#training_results_Norm=train(model_norm , criterion, train_loader, validation_loader, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-01e38e2830ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(sub_train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(sub_train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
