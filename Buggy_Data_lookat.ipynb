{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with dataset, no useful DNN in this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import seaborn as sb\n",
    "import matplotlib as mpl\n",
    "import h5py\n",
    "import pandas as pd \n",
    "import tables\n",
    "from pandas.compat import StringIO, BytesIO \n",
    "\n",
    "mpl.rcParams['figure.dpi']= 250\n",
    "pd.__file__\n",
    "\n",
    "from torch.utils import data\n",
    "#from hdf5dataset import HDF5Dataset\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import hdf5 file and look at what is inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open hdf5 file for reading\n",
    "    #hdf_buggy = pd.HDFStore('/data/wfedorko/top_tagging_spr1/subsampled/gen_spr1_full_zpdijet_cms_50pileup_notrim/flat_distribution2.hdf',mode='r') #can't access data using this - use h5py.File instead\n",
    "    #hdf_buggy.keys() #this is an empty array\n",
    "hdf_bug = h5py.File('/data/wfedorko/top_tagging_spr1/subsampled/gen_spr1_full_zpdijet_cms_50pileup_notrim/flat_distribution2.hdf',mode='r')\n",
    "\n",
    "print(list(hdf_bug.keys()))\n",
    "\n",
    "data = hdf_bug['data']\n",
    "print(data.shape)\n",
    "print(type(data))\n",
    "print(data[0][0])\n",
    "print(data[-1][0])\n",
    "\n",
    "sequence_len = hdf_bug['sequence lengths']\n",
    "print(sequence_len.shape)\n",
    "print(sequence_len[-1])\n",
    "\n",
    "#table_buggy = hdf_buggy.get('/table')\n",
    "\n",
    "#hdf_val = pd.HDFStore('val.h5',mode='r')\n",
    "#hdf_val.keys()\n",
    "#table_val = hdf.get('/table')\n",
    "\n",
    "#hdf_test = pd.HDFStore('test.h5',mode='r')\n",
    "#hdf_test.keys()\n",
    "#table_test = hdf.get('/table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to figure out what the stucture of the data set is and how many features are in each jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816\n",
      "[  0.        1.      185.94533]\n",
      "181.0\n"
     ]
    }
   ],
   "source": [
    "print(data[1].size)\n",
    "#print(list(data[1]))\n",
    "\n",
    "#print(data[1][0:17].size)\n",
    "print(data[1][0:3])\n",
    "#print(data[1][14])\n",
    "#print(data[1][20:27])\n",
    "\n",
    "print(sequence_len[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#still guessing...\n",
    "print('the number of features for constituent might be close to',2816/205)\n",
    "\n",
    "#max(sequence_len) is taking WAY too long, so here's a quick way to see what range of numbers sequence lengths contains\n",
    "for i in range(111150,111175):\n",
    "    print(sequence_len[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pytorch tensor\n",
    "train_tensor = torch.tensor(data)\n",
    "#val_tensor = torch.tensor(table_val.values)\n",
    "#test_tensor = torch.tensor(table_test.values)\n",
    "\n",
    "\n",
    "#len(torch_tensor) 1211000\n",
    "\n",
    "# another option, but this appears to be the same as using table...\n",
    "# d = pd.read_hdf('train.h5','table')\n",
    "# torch_tensor = torch.tensor(d.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not sure what this machine learning package below is, but I have kept it because it looks like it has some useful CNN architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_tensor,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "sub_train_loader = torch.utils.data.DataLoader(dataset=sub_train_tensor,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):\n",
    "        super(NetBatchNorm, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_size, n_hidden1)\n",
    "        self.linear2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.linear3 = nn.Linear(n_hidden2, out_size)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden1)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden2)\n",
    "        \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(torch.sigmoid(self.linear1(x)))\n",
    "        x = self.bn2(torch.sigmoid(self.linear2(x)))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "    # Activations, to analyze results \n",
    "    def activation(self, x):\n",
    "        out = []\n",
    "        z1 = self.bn1(self.linear1(x))\n",
    "        out.append(z1.detach().numpy().reshape(-1))\n",
    "        a1 = torch.sigmoid(z1)\n",
    "        out.append(a1.detach().numpy().reshape(-1).reshape(-1))\n",
    "        z2 = self.bn2(self.linear2(a1))\n",
    "        out.append(z2.detach().numpy().reshape(-1))\n",
    "        a2 = torch.sigmoid(z2)\n",
    "        out.append(a2.detach().numpy().reshape(-1))\n",
    "        return out\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_size, n_hidden1)\n",
    "        self.linear2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.linear3 = nn.Linear(n_hidden2, out_size)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "    # Activations, to analyze results \n",
    "    def activation(self, x):\n",
    "        out = []\n",
    "        z1 = self.linear1(x)\n",
    "        out.append(z1.detach().numpy().reshape(-1))\n",
    "        a1 = torch.sigmoid(z1)\n",
    "        out.append(a1.detach().numpy().reshape(-1).reshape(-1))\n",
    "        z2 = self.linear2(a1)\n",
    "        out.append(z2.detach().numpy().reshape(-1))\n",
    "        a2 = torch.sigmoid(z2)\n",
    "        out.append(a2.detach().numpy().reshape(-1))\n",
    "        return out \n",
    "    \n",
    "    \n",
    "def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n",
    "    i = 0\n",
    "    useful_stuff = {'training_loss':[], 'validation_accuracy':[]}  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x.view(-1, 28 * 28))\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            useful_stuff['training_loss'].append(loss.data.item())\n",
    "            \n",
    "        correct = 0\n",
    "        for x, y in validation_loader:\n",
    "            model.eval()\n",
    "            yhat = model(x.view(-1, 28 * 28))\n",
    "            _, label = torch.max(yhat, 1)\n",
    "            correct += (label == y).sum().item()\n",
    "            \n",
    "        accuracy = 100 * (correct / len(validation_dataset))\n",
    "        useful_stuff['validation_accuracy'].append(accuracy)\n",
    "    \n",
    "    return useful_stuff\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Data Loader for both train and validating\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_tensor, batch_size=2000, shuffle=True)\n",
    "#validation_loader = torch.utils.data.DataLoader(dataset=val_tensor, batch_size=5000, shuffle=False)\n",
    "\n",
    "\n",
    "sub_train_loader = torch.utils.data.DataLoader(dataset=sub_train_tensor,\n",
    "                                           batch_size=2000, \n",
    "                                           shuffle=True)\n",
    "\n",
    "type(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the criterion function\n",
    "criterion = nn.CrossEntropyLoss\n",
    "\n",
    "# Set the parameters\n",
    "input_dim = 12\n",
    "hidden_dim = 100\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
