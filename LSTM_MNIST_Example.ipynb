{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM MNIST Example\n",
    "\n",
    "\n",
    "## Started May 28, 2019\n",
    "### Genevieve Hayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data',train=True,transform=transforms.ToTensor(),download=True)\n",
    "test_dataset = dsets.MNIST(root='./data',train=False,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "train_len = len(train_dataset)\n",
    "test_len = len(test_dataset)\n",
    "\n",
    "print(train_len)\n",
    "print(test_len)\n",
    "\n",
    "# print(train_dataset.train_data.size())\n",
    "# print(train_dataset.train_labels.size())\n",
    "# print(test_dataset.test_data.size())\n",
    "# print(test_dataset.test_labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters/(len(train_dataset)/batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c60674fa6a9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "print(train_loader.size())\n",
    "print(len(test_loader))\n",
    "print(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LSTM Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel,self).__init__()\n",
    "        #Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        #Building the LSTM\n",
    "        #batch_first=True causes input/output tensors to be of shape (batch, seq_dim, feature)\n",
    "        self.lstm = nn.LSTM(input_dim,hidden_dim,layer_dim,batch_first=True)\n",
    "        \n",
    "        #Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim,x.size(0),self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        #Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim,x.size(0),self.hidden_dim).requires_grad_()\n",
    "        \n",
    "        #28 time steps\n",
    "        #We need to detach as we are doing truncates backpropagation through time (BPTT)\n",
    "        #If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn,cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        #Index hidden state of last time step\n",
    "        # out.size() --> 100,28,100\n",
    "        # out[:,-1,:] --> 100,100 --> only want hidden states of last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        #out.size() --> 100,10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#Instantiate Model Class\n",
    "########################\n",
    "\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#Instantiate Loss Class\n",
    "#######################\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "############################\n",
    "#Instantiate Optimizer Class\n",
    "############################\n",
    "\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 28])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "len(list(model.parameters())) \n",
    "#We have 6 groups of parameters comprising weights and biases from: \n",
    "#1. Input to Hidden Layer Affine Func, \n",
    "#2. Hidden Layer to Output Affine Func\n",
    "#3. Hidden Layer to Hidden Lay Affine Func\n",
    "\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n",
      "torch.Size([100, 28, 28])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6fcdc67780fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m#Load images as a torch tensor with gradient accumulation abilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;31m# PIL image mode: L, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;31m# unpack data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPixelAccess\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyAccess\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \"\"\"\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0;31m# realize palette\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputpalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#number of steps to unroll (28x28 size images)\n",
    "seq_dim = 28\n",
    "\n",
    "iter = 0\n",
    "train_loss = 0\n",
    "test_loss = 0\n",
    "train_corrects = 0\n",
    "test_corrects = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        #Load images as a torch tensor with gradient accumulation abilities\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        print(images.size())\n",
    "        #Clear gradients with respect to parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass to get output/logits\n",
    "        #outputs.size() --> 100,10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        #Getting gradients with respect to parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        #Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Store training loss and corrects\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "        train_corrects += loss.item()*images.size(0)\n",
    "        \n",
    "        iter += 1   \n",
    "        \n",
    "        if iter % 500 == 0:\n",
    "            #Calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            #Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                #Resize images\n",
    "                images = images.view(-1,seq_dim,input_dim)\n",
    "                \n",
    "                #Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "                \n",
    "                #Get predicted from the maximum value\n",
    "                _, predicted = torch.max(outputs.data,1)\n",
    "                \n",
    "                #Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                test_loss += loss.item()*images.size(0)\n",
    "            \n",
    "                #extract TP+TN\n",
    "                test_corrects += torch.sum(predicted == labels.data)\n",
    "                \n",
    "                #Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "                TESTaccuracy = 100*correct/total\n",
    "            \n",
    "            #Print loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter,loss.item(),TESTaccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2565d54e683f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#Forward pass to get output/logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#outputs.size() --> 100,10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#Calculate Loss: softmax --> cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7ef43536d82b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#We need to detach as we are doing truncates backpropagation through time (BPTT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#If we don't, we'll backprop all the way to the start even after going through another batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#Index hidden state of last time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#number of steps to unroll (28x28 size images)\n",
    "seq_dim = 28\n",
    "\n",
    "iter = 0\n",
    "train_loss = 0\n",
    "test_loss = 0\n",
    "train_corrects = 0\n",
    "test_corrects = 0\n",
    "correct = 0\n",
    "\n",
    "training_loss_list=[]\n",
    "test_loss_list=[]\n",
    "testing_loss_list=[]\n",
    "\n",
    "training_acc_list=[]\n",
    "test_acc_list = []\n",
    "testing_acc_list=[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if (epoch%1==10 or epoch==num_epochs - 1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs -1))\n",
    "        print('-' * 2)\n",
    "        \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_corrects = 0 \n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        #Load images as a torch tensor with gradient accumulation abilities\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        \n",
    "        #Clear gradients with respect to parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass to get output/logits\n",
    "        #outputs.size() --> 100,10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs,labels)\n",
    "        #Getting gradients with respect to parameters\n",
    "        loss.backward()\n",
    "        #Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Store training loss and corrects\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "        \n",
    "        train_corrects += loss.item()*images.size(0)\n",
    "        \n",
    "        total = labels.size(0)\n",
    "        \n",
    "        train_accuracy = 100*correct/total\n",
    "        \n",
    "        #training_acc_list.append(train_accuracy)\n",
    "        #train_loss.append(train_loss)\n",
    "        \n",
    "    iter += 1         \n",
    "    if iter % 500 == 0:\n",
    "        #Calculate Accuracy\n",
    "        test_corrects = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        #Iterate through test dataset\n",
    "        for images, labels in test_loader:\n",
    "        #Resize images\n",
    "            images = images.view(-1,seq_dim,input_dim)\n",
    "            \n",
    "            #Forward pass only to get logits/output\n",
    "            outputs = model(images)\n",
    "            \n",
    "            #Get predicted from the maximum value\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            \n",
    "            #Total number of labels\n",
    "            total += labels.size(0)                \n",
    "            test_loss += loss.item()*images.size(0)\n",
    "            \n",
    "            #Total correct predictions\n",
    "            correct += (predicted == labels).sum()\n",
    "                \n",
    "            TESTaccuracy = 100*correct/total\n",
    "                \n",
    "        #test_acc_list.append(TESTaccuracy)\n",
    "        #test_loss.append(test_loss)\n",
    "            \n",
    "        #Print loss\n",
    "        print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter,loss.item(),TESTaccuracy))\n",
    "            \n",
    "end_time = time.time()   \n",
    "print(\"--- {} seconds ---\".format(end_time-start_time)) \n",
    "print(\"--- {} min and {} sec ---\".format(math.floor((end_time-start_time)/60),math.ceil((end_time-start_time)%60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2814002470175425\n",
      "3.752638613382975\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bc7d39ba968a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#Forward pass to get output/logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#outputs.size() --> 100,10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#Calculate Loss: softmax --> cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7ef43536d82b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#We need to detach as we are doing truncates backpropagation through time (BPTT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#If we don't, we'll backprop all the way to the start even after going through another batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#Index hidden state of last time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "##########################################\n",
    "##########################################\n",
    "#train MNIST LSTM that MAKES SENSE\n",
    "\n",
    "#number of steps to unroll (28x28 size images)\n",
    "seq_dim = 28\n",
    "\n",
    "#initialize variables\n",
    "iter = 0\n",
    "train_loss = 0\n",
    "train_corrects = 0\n",
    "total = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        #Load images as torch tensor with gradient accumulation abilities\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "        \n",
    "        #Clear gradients with respect to parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass to get output/logits\n",
    "        #outputs.size() --> 100,10\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs,labels)\n",
    "        #Getting gradients with respect to parameters\n",
    "        loss.backward()\n",
    "        #Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Store the training loss and # of corrects and total number of events\n",
    "        train_loss += loss.item()*images.size(0)   \n",
    "        total = labels.size(0)\n",
    "        \n",
    "        #extract TP+TN\n",
    "        #train_corrects += torch.sum(outputs == labels.data)\n",
    "        \n",
    "    #epoch_loss = train_loss/train_len\n",
    "    #epoch_acc = train_corrects/train_len\n",
    "    \n",
    "    print(train_loss/train_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "print(labels.size())\n",
    "print(outputs.size())\n",
    "#outputs == labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "torch.Size([100, 10])\n",
      "torch.FloatTensor\n",
      "tensor([-2.2314, -2.7231, -4.6334, -0.4806,  1.7836, -0.3315, -5.1610,  1.3123,\n",
      "         0.7714,  9.8100], grad_fn=<SelectBackward>)\n",
      "4\n",
      "tensor([8, 9, 0, 1, 2, 7, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 7, 8, 9, 7, 8, 6, 9, 1,\n",
      "        9, 3, 8, 4, 4, 7, 0, 1, 9, 2, 8, 7, 8, 2, 6, 0, 6, 5, 3, 3, 3, 9, 1, 4,\n",
      "        0, 6, 1, 0, 0, 6, 2, 1, 1, 7, 7, 8, 4, 6, 0, 7, 0, 3, 6, 8, 7, 1, 5, 2,\n",
      "        4, 9, 4, 3, 6, 4, 1, 7, 2, 6, 5, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2,\n",
      "        3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs))\n",
    "print(outputs.size())\n",
    "print(outputs.type())\n",
    "print(outputs[1])\n",
    "\n",
    "indnp = np.argmax([1,2,3,4,5])\n",
    "#indnp = np.argmax(outputs)\n",
    "\n",
    "print(indnp)\n",
    "\n",
    "\n",
    "val,ind = torch.max(outputs,1)\n",
    "print(ind)\n",
    "#ind = outputs[1].max(0)\n",
    "#print(ind.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(images[1])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Trying to plot number along with model output\n",
    "\n",
    "# import numpy as np\n",
    "# # Create functions to graph the image along with confidence levels\n",
    "\n",
    "# #plot image function plots the image of interest along with prediction and truth labels\n",
    "# def plot_image(i,predictions_array, true_label,img):\n",
    "#     predictions_array,true_label,img = predictions_array[i], true_label[i], img[i]\n",
    "#     plt.grid(False)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "    \n",
    "#     plt.imshow(img, cmap=plt.cm.RdGy)\n",
    "\n",
    "#     #predicted_label = np.argmax(predictions_array)\n",
    "#     predicted_label = predictions_array.max(0)\n",
    "\n",
    "#     if predicted_label == true_label:\n",
    "#         color = 'green'\n",
    "#     else:\n",
    "#         color = 'red'\n",
    "        \n",
    "#     plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "#                                             100*np.max(predictions_array),\n",
    "#                                             class_names[true_label]),\n",
    "#                                             color = color)\n",
    "\n",
    "# #plot value function creates a bar graph of the confidence levels, colored according to correctness       \n",
    "# def plot_value_array(i,predictions_array,true_label):\n",
    "#     predictions_array,true_label = predictions_array[i], true_label[i]\n",
    "#     plt.grid(False)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     thisplot = plt.bar(range(10),predictions_array, color = \"#777777\")\n",
    "#     plt.ylim([0,1])\n",
    "#     #predicted_label = np.argmax(predictions_array)\n",
    "#     predicted_label = predictions_array.max(0)\n",
    "    \n",
    "#     thisplot[predicted_label].set_color('red')\n",
    "#     thisplot[true_label].set_color('green')\n",
    "\n",
    "\n",
    "\n",
    "# # plot one of them!\n",
    "# i = 4 \n",
    "# plt.figure(figsize=(6,3))\n",
    "# plt.subplot(1,2,1)\n",
    "# plot_image(i,outputs,labels,images)\n",
    "# #plt.subplot(1,2,2)\n",
    "# #plot_value_array(i,outputs,test_labels)\n",
    "# #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
