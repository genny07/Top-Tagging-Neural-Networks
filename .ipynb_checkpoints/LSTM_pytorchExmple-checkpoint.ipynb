{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Long-Short Term Memory (LSTM) Neural Network V1\n",
    "## Analysis of Gregor Dataset (Top Tagging)\n",
    "\n",
    "## Started May 27, 2019\n",
    "### Genevieve Hayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sb\n",
    "import matplotlib.pylab as plt\n",
    "#import matplotlib as mpl\n",
    "import h5py\n",
    "import tables\n",
    "import math\n",
    "import time\n",
    "import sklearn.metrics as sklm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_gpu=False\n",
    "if run_gpu:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Generating Data to explore LSTM inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TimeSeriesData:\n",
    "    def __init__(self, num_datapoints, test_size=0.2, max_t=20, num_prev=1,\n",
    "                 noise_var=1):\n",
    "        \"\"\"\n",
    "        Template class for generating time series data.\n",
    "        :param test_size: in (0,1), data to be used in test set as a fraction of all data generated.\n",
    "        \"\"\"\n",
    "        self.num_datapoints = num_datapoints\n",
    "        self.test_size = test_size\n",
    "        self.num_prev = num_prev\n",
    "        self.max_t = max_t\n",
    "        self.data = None\n",
    "        self.noise_var = noise_var\n",
    "        self.y = np.zeros(num_datapoints + num_prev*4) # TODO: check this\n",
    "        self.bayes_preds = np.copy(self.y)\n",
    "\n",
    "        # Generate data and reshape data\n",
    "        self.create_data()\n",
    "\n",
    "        # Split into training and test sets\n",
    "        self.train_test_split()\n",
    "\n",
    "    def create_data(self):\n",
    "        self.generate_data()\n",
    "        self.reshape_data()\n",
    "\n",
    "    def generate_data(self):\n",
    "        \"\"\"Generates data in self.y, may take as implicit input timesteps self.t.\n",
    "        May also generate Bayes predictions.\"\"\"\n",
    "        raise NotImplementedError(\"Generate data method not implemented.\")\n",
    "\n",
    "    def reshape_data(self):\n",
    "        self.x = np.reshape([self.y[i:i + self.num_prev] for i in range(\n",
    "            self.num_datapoints)], (-1, self.num_prev))\n",
    "        self.y = np.copy(self.y[self.num_prev:])\n",
    "        self.bayes_preds = np.copy(self.bayes_preds[self.num_prev:])\n",
    "\n",
    "    def train_test_split(self):\n",
    "        test_size = int(len(self.y) * self.test_size)\n",
    "        self.data = [self.X_train, self.X_test, self.y_train,\n",
    "                     self.y_test] = \\\n",
    "                    self.x[:-test_size], self.x[-test_size:], \\\n",
    "                    self.y[:-test_size], self.y[-test_size:]\n",
    "        self.bayes_preds = [self.bayes_train_preds, self.bayes_test_preds] = self.bayes_preds[:-test_size], self.bayes_preds[-test_size:]\n",
    "\n",
    "    def return_data(self):\n",
    "        return self.data\n",
    "\n",
    "    def return_train_test(self):\n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test\n",
    "\n",
    "class ARData(TimeSeriesData):\n",
    "    \"\"\"Class to generate autoregressive data.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, coeffs=None, **kwargs):\n",
    "        self.given_coeffs = coeffs\n",
    "        super(ARData, self).__init__(*args, **kwargs)\n",
    "\n",
    "        if coeffs is not None:\n",
    "            self.num_prev = len(coeffs) - 1\n",
    "\n",
    "    def generate_data(self):\n",
    "        self.generate_coefficients()\n",
    "        self.generate_initial_points()\n",
    "\n",
    "        # + 3*self.num_prev because we want to cut first (3*self.num_prev) datapoints later\n",
    "        # so dist is more stationary (else initial num_prev datapoints will stand out as diff dist)\n",
    "        for i in range(self.num_datapoints+3*self.num_prev):\n",
    "            # Generate y value if there was no noise\n",
    "            # (equivalent to Bayes predictions: predictions from oracle that knows true parameters (coefficients))\n",
    "            self.bayes_preds[i + self.num_prev] = np.dot(self.y[i:self.num_prev+i][::-1], self.coeffs)\n",
    "            # Add noise\n",
    "            self.y[i + self.num_prev] = self.bayes_preds[i + self.num_prev] + self.noise()\n",
    "\n",
    "        # Cut first 20 points so dist is roughly stationary\n",
    "        self.bayes_preds = self.bayes_preds[3*self.num_prev:]\n",
    "        self.y = self.y[3*self.num_prev:]\n",
    "\n",
    "    def generate_coefficients(self):\n",
    "        if self.given_coeffs is not None:\n",
    "            self.coeffs = self.given_coeffs\n",
    "        else:\n",
    "            filter_stable = False\n",
    "            # Keep generating coefficients until we come across a set of coefficients\n",
    "            # that correspond to stable poles\n",
    "            while not filter_stable:\n",
    "                true_theta = np.random.random(self.num_prev) - 0.5\n",
    "                coefficients = np.append(1, -true_theta)\n",
    "                # check if magnitude of all poles is less than one\n",
    "                if np.max(np.abs(np.roots(coefficients))) < 1:\n",
    "                    filter_stable = True\n",
    "            self.coeffs = true_theta\n",
    "\n",
    "    def generate_initial_points(self):\n",
    "        # Initial datapoints distributed as N(0,1)\n",
    "        self.y[:self.num_prev] = np.random.randn(self.num_prev)\n",
    "\n",
    "    def noise(self):\n",
    "        # Noise distributed as N(0, self.noise_var)\n",
    "        return self.noise_var * np.random.randn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Set parameters\n",
    "#####################\n",
    "\n",
    "# Data params\n",
    "noise_var = 0\n",
    "num_datapoints = 100\n",
    "test_size = 0.2\n",
    "num_train = int((1-test_size) * num_datapoints)\n",
    "\n",
    "# Network params\n",
    "input_size = 20\n",
    "# If `per_element` is True, then LSTM reads in one timestep at a time.\n",
    "per_element = True\n",
    "if per_element:\n",
    "    lstm_input_size = 1\n",
    "else:\n",
    "    lstm_input_size = input_size\n",
    "# size of hidden layers\n",
    "h1 = 32\n",
    "output_dim = 1\n",
    "num_layers = 2\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 500\n",
    "dtype = torch.float\n",
    "\n",
    "#####################\n",
    "# Generate data\n",
    "#####################\n",
    "data = ARData(num_datapoints, num_prev=input_size, test_size=test_size, noise_var=noise_var)\n",
    "\n",
    "# make training and test sets in torch\n",
    "X_train = torch.from_numpy(data.X_train).type(torch.Tensor)\n",
    "X_test = torch.from_numpy(data.X_test).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(data.y_train).type(torch.Tensor).view(-1)\n",
    "y_test = torch.from_numpy(data.y_test).type(torch.Tensor).view(-1)\n",
    "\n",
    "X_train = X_train.view([input_size, -1, 1])\n",
    "X_test = X_test.view([input_size, -1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # Set Data Parameters\n",
    "# #######################\n",
    "# # Data params\n",
    "# noise_var = 0\n",
    "# num_datapoints = 100\n",
    "# test_size = 0.2\n",
    "# num_train = int((1-test_size) * num_datapoints)\n",
    "\n",
    "# # Network params\n",
    "# input_size = 800 #input dimension\n",
    "# # If `per_element` is True, then LSTM reads in one timestep at a time.\n",
    "# per_element = True\n",
    "# if per_element:\n",
    "#     lstm_input_size = 1\n",
    "# else:\n",
    "#     lstm_input_size = input_size\n",
    "\n",
    "# num_epochs = 10\n",
    "# batch_size = 30 \n",
    "# learning_rate = 0.001\n",
    "\n",
    "# h1 = 100 #size of hidden layer\n",
    "# output_dim = 1\n",
    "# num_layers = 2\n",
    "\n",
    "# #####################\n",
    "# # Generate data\n",
    "# #####################\n",
    "# data = ARData(num_datapoints, num_prev=input_size, test_size=test_size, noise_var=noise_var, coeffs=fixed_ar_coefficients[input_size])\n",
    "\n",
    "# # make training and test sets in torch\n",
    "# X_train = torch.from_numpy(data.X_train).type(torch.Tensor)\n",
    "# X_test = torch.from_numpy(data.X_test).type(torch.Tensor)\n",
    "# y_train = torch.from_numpy(data.y_train).type(torch.Tensor).view(-1)\n",
    "# y_test = torch.from_numpy(data.y_test).type(torch.Tensor).view(-1)\n",
    "\n",
    "# X_train = X_train.view([input_size, -1, 1])\n",
    "# X_test = X_test.view([input_size, -1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Training dataset loading class\n",
    "class TrainDataset(Dataset):\n",
    "    # Initialize our data, download, etc.  \n",
    "    def __init__(self):\n",
    "        #open hdf5 file for reading\n",
    "        hdf_train = pd.HDFStore('train.h5',mode='r')\n",
    "        hdf_train.keys()\n",
    "        table_train = hdf_train.get('/table')\n",
    "        \n",
    "        #numpy representation of DataFrame\n",
    "        array_train = table_train.values\n",
    "        \n",
    "        self.len = array_train.shape[0]\n",
    "        self.x_data = torch.from_numpy(array_train[:,0:800]).float() #x data is only the 4 vectors of the constituents \n",
    "        self.y_data = torch.from_numpy(array_train[:,-1]).long() #y data is only the labels (0=QCD, 1=top)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index],self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "# Validation dataset loading class\n",
    "class ValDataset(Dataset):\n",
    "    # Initialize our data, download, etc.\n",
    "    def __init__(self):\n",
    "        #open hdf5 file for reading\n",
    "        hdf_val = pd.HDFStore('val.h5',mode='r')\n",
    "        hdf_val.keys()\n",
    "        table_val = hdf_val.get('/table')\n",
    "        \n",
    "        #numpy representation of DataFrame\n",
    "        array_val = table_val.values\n",
    "        \n",
    "        self.len = array_val.shape[0]\n",
    "        self.x_data = torch.from_numpy(array_val[:,0:800]).float()\n",
    "        self.y_data = torch.from_numpy(array_val[:,-1]).long()\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index],self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = TrainDataset()\n",
    "validation_dataset = ValDataset()\n",
    "\n",
    "train_len = training_dataset.len\n",
    "val_len = validation_dataset.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Set Parameters\n",
    "#######################\n",
    "num_epochs = 10\n",
    "batch_size = 30 \n",
    "learning_rate = 0.001\n",
    "\n",
    "lstm_input_size = 800 #input dimension\n",
    "h1 = 100 #size of hidden layer\n",
    "output_dim = 1\n",
    "num_layers = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Build Model\n",
    "#######################\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1, num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        #Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "        \n",
    "        #Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "        \n",
    "    #initialize hidden states\n",
    "    def init_hidden(self):\n",
    "         return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim) \\\n",
    "                    ,torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both have shape (num_layers,batch_size, hidden_dim)\n",
    "        \n",
    "        #lstm_out, self.hidden = self.lstm(input.view(len(input),self.batch_size, -1))\n",
    "        #lstm_out, self.hidden = self.lstm(input.view(input[1,:].size()),self.batch_size, -1)\n",
    "        lstm_out, self.hidden = self.lstm(input.view(input[0,:].size()),self.batch_size, -1)\n",
    "        #lstm_out, self.hidden = self.lstm(input.view(input[1,:].size(),self.batch_size))\n",
    "        \n",
    "        #Only take the output from the final timestep\n",
    "        #If seq2seq prediction, can pass on the entirety of lstm_out to the next layer\n",
    "        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred.view(-1)\n",
    "        y_pred = self.linear(lstm_out[-1].view(self.batch_size))\n",
    "        #return y_pred\n",
    "        \n",
    "model = LSTM(lstm_input_size, h1, batch_size=batch_size, output_dim=output_dim, num_layers=num_layers)    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define flattening function\n",
    "def flatten(t):\n",
    "    t = t.reshape(1, -1)\n",
    "    t = t.squeeze()\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes from 2 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-48c6f2b6c505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#         #Forward pass: Compute predicted y by passing x to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#         preds=torch.max(outputs.data,1)[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-177-819fc6a3c7cd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#lstm_out, self.hidden = self.lstm(input.view(len(input),self.batch_size, -1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#lstm_out, self.hidden = self.lstm(input.view(input[1,:].size()),self.batch_size, -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m#lstm_out, self.hidden = self.lstm(input.view(input[1,:].size(),self.batch_size))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes from 2 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# make dataloaders\n",
    "training_loader = DataLoader(dataset=training_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "validation_loader = DataLoader(dataset=validation_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "for i, data in enumerate(training_loader, 0):\n",
    "        ### 1. get the inputs ###\n",
    "        inputs,labels=data\n",
    "        \n",
    "        ### 2. wrap them in Variable ###\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        #inputs = flatten(inputs)\n",
    "        \n",
    "#         #Forward pass: Compute predicted y by passing x to the model\n",
    "        outputs=model(inputs)\n",
    "#         preds=torch.max(outputs.data,1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 800])\n",
      "<built-in method size of Tensor object at 0x7f208a08c120>\n",
      "<built-in method size of Tensor object at 0x7f208a08c120>\n"
     ]
    }
   ],
   "source": [
    "print(inputs.size())\n",
    "\n",
    "print(inputs[:,0].size)\n",
    "print(inputs[0,:].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a41ff8661ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# Train Model\n",
    "#######################\n",
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    #Clear stored gradient\n",
    "    model.zero_grad()\n",
    "    \n",
    "    #Initialize hidden state\n",
    "    # **Don't do this if you want LSTM to be stateful**\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    #Forward pass\n",
    "    y_pred = model(training_dataset)\n",
    "    \n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    if t % 10 == 0:\n",
    "        print(\"Epoch \",t,\"MSE: \", loss.item())\n",
    "    \n",
    "    hist[t] = loss.item()\n",
    "    \n",
    "    \n",
    "    #Zero out gradient, else they will accumulate between epochs\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    #Update parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
